{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "name": "Team_8_Final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8T3_GVOk4eMY"
      },
      "source": [
        "# importing attention layer from external package\n",
        "import attention\n",
        "from attention import AttentionLayer\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer \n",
        "from keras.preprocessing.sequence import pad_sequences \n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import Attention\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKaAsh1d4eMb",
        "outputId": "e4f7c604-a39e-4868-8465-942492b5b02d"
      },
      "source": [
        "# COnfiguration For Training the Model on GPU with CUDA\n",
        "\n",
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "def get_available_devices():\n",
        "    local_device_protos = device_lib.list_local_devices()\n",
        "    return [x.name for x in local_device_protos]\n",
        "\n",
        "print(get_available_devices())\n",
        "\n",
        "devices = get_available_devices()\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "print(physical_devices)\n",
        "\n",
        "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['/device:CPU:0', '/device:XLA_CPU:0', '/device:GPU:0', '/device:XLA_GPU:0']\n",
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k57gU1884eMd"
      },
      "source": [
        "#  Import the data from file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6XY0DC-4eMd"
      },
      "source": [
        "# We are taking only 100000 rows of the data set for training\n",
        "with tf.device(devices[2]):\n",
        "    df = pd.read_csv('../archive/Reviews.csv',nrows = 200000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8SrkcPr4eMe"
      },
      "source": [
        "# removing Null, Duplicates rows from data frame\n",
        "with tf.device(devices[2]):\n",
        "    df.isnull().sum()\n",
        "    df.drop_duplicates(subset=['Text'],inplace=True)\n",
        "    df = df[df['Summary'].notna()]\n",
        "    df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOJd1e7y4eMf"
      },
      "source": [
        "# Keep Only two rows from the data frame and dropping all others\n",
        "with tf.device(devices[2]):\n",
        "    df=df[['Summary','Text']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RQIhUOV4eMf"
      },
      "source": [
        "# Make dictionary of contractions words of the english \n",
        "with tf.device(devices[2]):\n",
        "    contractions = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\",\n",
        "                    \"could've\": \"could have\", \"couldn't\": \"could not\",\"didn't\": \"did not\",  \"doesn't\": \"does not\",\n",
        "                    \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "                    \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \n",
        "                    \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "                    \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\n",
        "                    \"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
        "                    \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\",\n",
        "                    \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "                    \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\":\n",
        "                    \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
        "                    \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\":\n",
        "                    \"might not have\", \"must've\": \"must have\",\"mustn't\": \"must not\", \"mustn't've\":\n",
        "                    \"must not have\", \"needn't\": \"need not\",\"needn't've\":\n",
        "                    \"need not have\",\"o'clock\": \"of the clock\",\n",
        "                    \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n",
        "                    \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
        "                    \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\",\n",
        "                    \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
        "                    \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\n",
        "                    \"so've\": \"so have\",\"so's\": \"so as\",\n",
        "                    \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "                    \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\n",
        "                    \"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "                    \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\",\n",
        "                    \"they've\": \"they have\", \"to've\": \"to have\",\n",
        "                    \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
        "                    \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\",\n",
        "                    \"what're\": \"what are\",\n",
        "                    \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
        "                    \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
        "                    \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
        "                    \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
        "                    \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "                    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
        "                    \"you're\": \"you are\", \"you've\": \"you have\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKM7gShP4eMi"
      },
      "source": [
        "# Pre-processing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvRbVVWi4eMi"
      },
      "source": [
        "with tf.device(devices[2]):\n",
        "    \n",
        "    summary = df['Summary']\n",
        "    text = df['Text']\n",
        "\n",
        "    def func(text, flag):\n",
        "        # text is the original summary and original text data which will be input.\n",
        "        # If Flag is 'S' then it's summary and we won't remove stop words from that otherwise we'll remove Stop words from the text\n",
        "        \n",
        "        # convert to lower case \n",
        "        text = text.apply(lambda x: x.lower())\n",
        "        \n",
        "        # remove contractions\n",
        "        for i in range(len(text)):\n",
        "            try:\n",
        "                sent = text[i]\n",
        "                words = []\n",
        "                for j in sent.split(\" \"):\n",
        "                    if j in contractions:\n",
        "                        words.append(contractions[j])\n",
        "                    else:\n",
        "                        words.append(j)\n",
        "                text[i] = ' '.join(words)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        # remove numbers, unwanted space, punctuations etc. \n",
        "        text = text.apply(lambda x: re.sub(\"[^a-zA-Z]\", \" \", x))\n",
        "        text = text.apply(lambda x: re.sub('[m]{2,}', 'mm', x))\n",
        "        text = text.apply(lambda x: x.replace('[^\\w\\s]',''))\n",
        "        text = text.apply(lambda x: re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', x, flags=re.MULTILINE))\n",
        "        text = text.apply(lambda x: re.sub(r'\\<a href', ' ', x))\n",
        "        text = text.apply(lambda x: re.sub(r'&amp;', '', x))\n",
        "        text = text.apply(lambda x: re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', x))\n",
        "        text = text.apply(lambda x: re.sub(r'<br />', ' ', x))\n",
        "        text = text.apply(lambda x: re.sub(r'\\'', ' ', x))\n",
        "\n",
        "\n",
        "        # making set of all stop words\n",
        "        stop = set(stopwords.words('english'))\n",
        "        \n",
        "        # remove stop words from text\n",
        "        if flag != 'S':\n",
        "            text= text.apply(lambda x:' '.join([word for word in x.split(' ') if word not in (stop)]))\n",
        "\n",
        "\n",
        "        text = text.apply(lambda x:re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", x))\n",
        "        text = text.apply(lambda x:re.sub(' +', ' ',x))\n",
        "\n",
        "        return text\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # calling func fro text \n",
        "    text = func(text, 'T')\n",
        "\n",
        "\n",
        "    # calling func fro Summary\n",
        "    summary = func(summary, 'S')\n",
        "\n",
        "    \n",
        "    # making data frame again\n",
        "    df['Summary'] = summary\n",
        "    df['Text'] = text\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBNpyZ0F4eMn",
        "outputId": "83b78ed0-969c-4bfe-a8f1-faabbe17073e"
      },
      "source": [
        "# Printing Cleaned Summary\n",
        "with tf.device(devices[2]):\n",
        "    print(\"Original Reviews\")\n",
        "    for i in range(10):\n",
        "        print(str(i) + \" \" + df['Text'][i])\n",
        "        \n",
        "    print()\n",
        "    print(\"Original Summaries\")\n",
        "    for i in range(10):\n",
        "        print(str(i) + \" \" + df['Summary'][i])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original Reviews\n",
            "0 bought several vitality canned dog food products found good quality product looks like stew processed meat smells better labrador finicky appreciates product better \n",
            "1 product arrived labeled jumbo salted peanuts peanuts actually small sized unsalted sure error vendor intended represent product jumbo \n",
            "2 confection around centuries light pillowy citrus gelatin nuts case filberts cut tiny squares liberally coated powdered sugar tiny mouthful heaven chewy flavorful highly recommend yummy treat familiar story c lewis lion witch wardrobe treat seduces edmund selling brother sisters witch \n",
            "3 looking secret ingredient robitussin believe found got addition root beer extract ordered good made cherry soda flavor medicinal \n",
            "4 great taffy great price wide assortment yummy taffy delivery quick taffy lover deal \n",
            "5 got wild hair taffy ordered five pound bag taffy enjoyable many flavors watermelon root beer melon peppermint grape etc complaint bit much red black licorice flavored pieces particular favorites kids husband lasted two weeks would recommend brand taffy delightful treat \n",
            "6 saltwater taffy great flavors soft chewy candy individually wrapped well none candies stuck together happen expensive version fralinger would highly recommend candy served beach themed party everyone loved \n",
            "7 taffy good soft chewy flavors amazing would definitely recommend buying satisfying \n",
            "8 right mostly sprouting cats eat grass love rotate around wheatgrass rye\n",
            "9 healthy dog food good digestion also good small puppies dog eats required amount every feeding \n",
            "\n",
            "Original Summaries\n",
            "0 good quality dog food\n",
            "1 not as advertised\n",
            "2  delight says it all\n",
            "3 cough medicine\n",
            "4 great taffy\n",
            "5 nice taffy\n",
            "6 great just as good as the expensive brands \n",
            "7 wonderful tasty taffy\n",
            "8 yay barley\n",
            "9 healthy dog food\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qz_wZ0z84eMq"
      },
      "source": [
        "with tf.device(devices[2]):\n",
        "    df.replace('', np.nan, inplace=True)\n",
        "    df.dropna(axis=0,inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zWSXrUa4eMr",
        "outputId": "558a9e64-5e6b-425f-a694-7f1f387a5ac0"
      },
      "source": [
        "# Counting Summaries with length less than 8 and Text with length less than 50\n",
        "with tf.device(devices[2]):\n",
        "    print(len(df))\n",
        "\n",
        "    count = 0\n",
        "    for i in df['Summary']:\n",
        "        if(len(i.split())<=8):\n",
        "            count += 1\n",
        "    print(\"Total rows with summary length <=8 are = \", count)\n",
        "    print(count/len(df['Summary']))\n",
        "\n",
        "    \n",
        "###### idhr change kra h 80 50 ki jagah (Kr skte h ek bar train 80 pr bhi )\n",
        "\n",
        "    count = 0\n",
        "    for i in df['Text']:\n",
        "        if(len(i.split())<=50):\n",
        "            count += 1\n",
        "    print(\"Total rows with Text length <=50 are = \", count)\n",
        "    print(count)\n",
        "    print(count/len(df['Text']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "162841\n",
            "Total rows with summary length <=8 are =  151690\n",
            "0.9315221596526674\n",
            "Total rows with Text length <=50 are =  122859\n",
            "122859\n",
            "0.7544721538187558\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWuVNE304eMr",
        "outputId": "c4418844-1c15-412b-a8c4-4839d7bc902c"
      },
      "source": [
        "# Decarding rows which has summary or Text length below or above the threesold\n",
        "\n",
        "with tf.device(devices[2]):\n",
        "\n",
        "    sorted_summary = []\n",
        "    sorted_text = []\n",
        "    min_summary_length = 1\n",
        "    max_summary_length = 8\n",
        "    min_text_length = 5\n",
        "    max_text_length = 50\n",
        "    \n",
        "\n",
        "    for i in range(len(text)):\n",
        "        try:\n",
        "\n",
        "            text_length = len(text[i].split(' '))\n",
        "            summary_length = len(summary[i].split(' '))\n",
        "\n",
        "            if((text_length >= min_text_length) and (text_length<=max_text_length)\n",
        "              and summary_length >= min_summary_length and summary_length<=max_summary_length):\n",
        "                sorted_summary.append(summary[i])\n",
        "                sorted_text.append(text[i])\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "\n",
        "    summary = sorted_summary\n",
        "    text = sorted_text\n",
        "\n",
        "    print(len(summary))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "96706\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JowJJyns4eMs"
      },
      "source": [
        "# Adding start and end tokens in the summary (It will be used in Decoding the output of the sequence in last step)\n",
        "\n",
        "with tf.device(devices[2]):\n",
        "    df = pd.DataFrame({'Text' : text, 'Summary' : summary})\n",
        "\n",
        "\n",
        "    df['Summary'] = df['Summary'].apply(lambda x : 'strtok ' + x + ' endtok')\n",
        "    summary = df['Summary']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuC4CYL44eMs",
        "outputId": "6c0a63d8-6ddf-4785-f89a-8f456a73258d"
      },
      "source": [
        "# counting the number of Unique words in both text and summaries and also counting rare words with freequency less tham threshold\n",
        "\n",
        "def count_words(text):\n",
        "    words = dict()\n",
        "    for sentence in text:\n",
        "        for curr in sentence.split(' '):\n",
        "            if curr in words:\n",
        "                words[curr]+=1\n",
        "            else:\n",
        "                words[curr] = 1\n",
        "    return words\n",
        "\n",
        "def rare_count_words( words, threshold ):\n",
        "    rare_words = 0\n",
        "    for word in words:\n",
        "        if words[word]<threshold :\n",
        "            rare_words = rare_words+1\n",
        "    return rare_words  \n",
        "\n",
        "\n",
        "with tf.device(devices[2]):\n",
        "\n",
        "    unique_text_words = count_words(text)\n",
        "    unique_summary_words = count_words(summary)\n",
        "\n",
        "    unique_text_words_count = len(unique_text_words)\n",
        "    unique_summary_words_count = len(unique_summary_words)\n",
        "\n",
        "\n",
        "    # count_words is total number of unique words \n",
        "\n",
        "    # rare words are total words which are below threshold\n",
        "\n",
        "    text_threshold = 4\n",
        "    summary_threshold = 6\n",
        "\n",
        "    rare_text_words_count = rare_count_words(unique_text_words,text_threshold)\n",
        "    rare_summary_words_count = rare_count_words(unique_summary_words, summary_threshold)\n",
        "\n",
        "\n",
        "\n",
        "    print(\" Total Unique words in Text : \", unique_text_words_count)\n",
        "    print(\" Total Unique words in Summary : \", unique_summary_words_count)\n",
        "    print(\" Total rare words in Text : \", rare_text_words_count)\n",
        "    print(\" Total rare words in Summary : \", rare_summary_words_count)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Total Unique words in Text :  40714\n",
            " Total Unique words in Summary :  14362\n",
            " Total rare words in Text :  26079\n",
            " Total rare words in Summary :  10982\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5t-9SUmk4eMt"
      },
      "source": [
        "# Tokenizing the text to convert words into integers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Y5PARnB4eMt",
        "outputId": "e508c653-0fdc-4654-fc24-5f199229a481"
      },
      "source": [
        "with tf.device(devices[2]):\n",
        "    \n",
        "    # create tokenizer for Text and we will be assigning integers only to important word(i.e descard rare words)\n",
        "    # num_words argument in Toeknizer function will discard rare words \n",
        "    \n",
        "    text_tokenizer = Tokenizer(num_words  = (unique_text_words_count - rare_text_words_count))\n",
        "    text_tokenizer.fit_on_texts(text)\n",
        "\n",
        "    # this will be Interger sequences of all the text data \n",
        "    text_sequences = text_tokenizer.texts_to_sequences(text)\n",
        "    text_data = pad_sequences(text_sequences, maxlen= max_text_length , padding= 'post')\n",
        "\n",
        "    text = text_data\n",
        "\n",
        "\n",
        "\n",
        "    # create tokenizer for Text and we will be assigning integers only to important word(i.e descard rare words)\n",
        "    # num_words argument in Toeknizer function will discard rare words \n",
        "\n",
        "    \n",
        "################## + 2 mi ayega idhr \n",
        "\n",
        "    summary_tokenizer = Tokenizer(num_words  = unique_summary_words_count - rare_summary_words_count + 2)\n",
        "    summary_tokenizer.fit_on_texts(summary)\n",
        "\n",
        "    # this will be Interger sequences of all the summaries\n",
        "    summary_sequences = summary_tokenizer.texts_to_sequences(summary)\n",
        "    summary_data = pad_sequences(summary_sequences, maxlen= max_summary_length , padding= 'post')\n",
        "\n",
        "    summary = summary_data\n",
        "\n",
        "\n",
        "    # text_vocab_length will be total words present into text after discarding rare words and same with summary_vocab_length\n",
        "    \n",
        "    text_vocab_length = text_tokenizer.num_words + 1\n",
        "    summary_vocab_length = summary_tokenizer.num_words + 1\n",
        "\n",
        "\n",
        "    print(text_vocab_length)\n",
        "    print(summary_vocab_length)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14636\n",
            "3383\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G725mrou4eMt"
      },
      "source": [
        "# Deleting the rows which has only 2 words (i.e. strtok and endtok) beacuase there may be some summaries \n",
        "# with only rare words and that may become empty after tokenize\n",
        "\n",
        "with tf.device(devices[2]):\n",
        "    indexes=[]\n",
        "    for i in range(len(summary)):\n",
        "        count=0\n",
        "        for curr_value in summary[i]:\n",
        "            if curr_value != 0:\n",
        "                count=count+1\n",
        "        if(count==2):\n",
        "            indexes.append(i)\n",
        "\n",
        "    # delete the rows which are there in indexes list \n",
        "    summary =np.delete(summary,indexes, axis=0)\n",
        "    text=np.delete(text,indexes, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KD2UuiOx4eMu",
        "outputId": "f292e4b9-20a3-44d3-9bf3-6b91634fbae6"
      },
      "source": [
        "from keras import backend as K \n",
        "K.clear_session()\n",
        "\n",
        "# total number of LSTM cells in one layer of LSTM\n",
        "LSTM_CELLS_COUNT = 500\n",
        "\n",
        "# Length of feature vector/ total features considered for each word in the embedding matrix\n",
        "TEXT_EMBEDDING_DIM = 300\n",
        "\n",
        "\n",
        "with tf.device(devices[2]):\n",
        "    \n",
        "    # Shape of encoder Inputs \n",
        "    encoder_inputs = Input(shape=(max_text_length,))\n",
        "    \n",
        "    # Embedding matrix for encoder which gives input to the encoder according to input vector\n",
        "    encoder_embedding_layer = Embedding(text_vocab_length, TEXT_EMBEDDING_DIM,trainable=True)(encoder_inputs)\n",
        "\n",
        "    \n",
        "    # Stacked LSTM layers \n",
        "\n",
        "    # encoder layer 1 which will corresponding embedding matrix as a input \n",
        "    encoder_lstm_layer1 = LSTM(LSTM_CELLS_COUNT, return_sequences=True, return_state=True,  dropout = 0.5,activation = 'tanh',recurrent_activation = 'sigmoid',recurrent_dropout = 0,unroll = False,use_bias = True)\n",
        "    # Outputs of LSTM layer1\n",
        "    encoder_output1, state_h_layer1, state_c_layer1 = encoder_lstm_layer1(encoder_embedding_layer)\n",
        "\n",
        "    # encoder layer 2\n",
        "    encoder_lstm_layer2 = LSTM(LSTM_CELLS_COUNT, return_sequences=True, return_state=True, dropout = 0.5,activation = 'tanh',recurrent_activation = 'sigmoid',recurrent_dropout = 0,unroll = False,use_bias = True)\n",
        "    # Outputs of LSTM layer2\n",
        "    encoder_output2, state_h_layer2, state_c_layer2 = encoder_lstm_layer2(encoder_output1)\n",
        "\n",
        "    # encoder layer 3\n",
        "    encoder_lstm_layer3 = LSTM(LSTM_CELLS_COUNT, return_sequences=True, return_state=True, dropout = 0.5,activation = 'tanh',recurrent_activation = 'sigmoid',recurrent_dropout = 0,unroll = False,use_bias = True )\n",
        "    # Outputs of LSTM layer1\n",
        "    encoder_output3, state_h_layer3, state_c_layer3 = encoder_lstm_layer3(encoder_output2)\n",
        "\n",
        "\n",
        "    #decoder \n",
        "    decoder_inputs = Input(shape=(None,))\n",
        "    \n",
        "    # Embedding Matrix of decoder \n",
        "    embedding_layer_decoder = Embedding(summary_vocab_length, TEXT_EMBEDDING_DIM,trainable=True)\n",
        "    decoder_embedding = embedding_layer_decoder(decoder_inputs)\n",
        "\n",
        "    \n",
        "    # LSTM layer of decoder layer \n",
        "    decoder_lstm = LSTM(LSTM_CELLS_COUNT, return_sequences=True, return_state=True, dropout = 0.5,activation = 'tanh',recurrent_activation = 'sigmoid',recurrent_dropout = 0,unroll = False,use_bias = True)\n",
        "    decoder_output, fw_state, bw_state = decoder_lstm(decoder_embedding, initial_state = [state_h_layer3, state_c_layer3] )\n",
        "\n",
        "    #attention layer\n",
        "\n",
        "    attn_layer = AttentionLayer(name='attention_layer')\n",
        "    attention_out, x = attn_layer([encoder_output3, decoder_output])\n",
        "\n",
        "    \n",
        "    decoder_attention_concat = Concatenate(axis = -1, name = \"concat_layer\")([decoder_output, attention_out])\n",
        "\n",
        "\n",
        "    # dense layer to apply softmaxx activation function and its outputs will be final outputs \n",
        "\n",
        "    decoder_dense = TimeDistributed(Dense(summary_vocab_length, activation = 'softmax'))\n",
        "    decoder_output = decoder_dense(decoder_attention_concat)\n",
        "\n",
        "\n",
        "    model = Model( [encoder_inputs, decoder_inputs],  decoder_output)\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 50)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 50, 300)      4390800     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(None, 50, 500), (N 1602000     embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, 50, 500), (N 2002000     lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 300)    1014900     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   [(None, 50, 500), (N 2002000     lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   [(None, None, 500),  1602000     embedding_1[0][0]                \n",
            "                                                                 lstm_2[0][1]                     \n",
            "                                                                 lstm_2[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "attention_layer (AttentionLayer ((None, None, 500),  500500      lstm_2[0][0]                     \n",
            "                                                                 lstm_3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concat_layer (Concatenate)      (None, None, 1000)   0           lstm_3[0][0]                     \n",
            "                                                                 attention_layer[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed (TimeDistribut (None, None, 3383)   3386383     concat_layer[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 16,500,583\n",
            "Trainable params: 16,500,583\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNqQZNnm4eMu"
      },
      "source": [
        "# compile the model with correct optimizer and loss function\n",
        "\n",
        "with tf.device(devices[2]):\n",
        "    model.compile(optimizer = 'rmsprop', loss = 'sparse_categorical_crossentropy')\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', mode = 'min',  patience=2, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhZKuNC84eMu"
      },
      "source": [
        "# Split the data into training and testing parts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xmt-EFS4eMv"
      },
      "source": [
        "# Spilt tha data into test and train sets\n",
        "with tf.device(devices[2]):\n",
        "    text_train, text_test, summary_train, summary_test = train_test_split(np.array(text), np.array(summary), test_size = 0.1, random_state = 0, shuffle = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLr34l4H4eMv",
        "outputId": "6fda64cd-2852-4cd0-cfdb-4af0a5963964"
      },
      "source": [
        "with tf.device(devices[2]):\n",
        "    print(text_train[:10])\n",
        "    print(summary_train[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 190  811   30   31  197  544 1128  176  235    3  622  104 1303  209\n",
            "    11    6    4   40  552  149  688  633 1803  211  817  441  132  176\n",
            "    46    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0]\n",
            " [   8 1723   46   49 1502   25   20    7   68  491  857   78  184 1677\n",
            "    62   33   46   49  297  809   46   49  297    7 6153   10   17 1395\n",
            "    16   13    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0]\n",
            " [   4    5   36  656   16  191  160   70   40    2  340 2542  126    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0]\n",
            " [  25  210 1186 3624 1022  869  177    2    9 1468  639 3703  284  634\n",
            "    71  869 2260 3693 2962    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0]\n",
            " [  52   60    6  572  267  396  157    7  454  131  437    2 2102  783\n",
            "  1175  295 1331 2255  203    7  514 5452  143  598    9 8510  541 6125\n",
            "    43   32   26  397   50    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0]\n",
            " [  53  415   94 1675    1    1  217    7    7 1402 1305   32 2672  494\n",
            "  1760    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0]\n",
            " [ 118  468  709 2609  492   12 1225  599   22   17 3210   98   57 1414\n",
            "  1676  188  179  289   10  382  407    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0]\n",
            " [ 347  283   36 1386  283  105    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0]\n",
            " [ 275  144  266  523  117  313  234  233    4 1808   63 3487    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0]\n",
            " [ 329   66  241   28 5627  241   32 1412  659  339  257 3035    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0]]\n",
            "[[   1 1101  222    2    0    0    0    0]\n",
            " [   1   83   26   14  577    2    0    0]\n",
            " [   1    5    6    2    0    0    0    0]\n",
            " [   1  204   33   13    2    0    0    0]\n",
            " [   1  115 1422   22   37  165  193    2]\n",
            " [   1    3   12    2    0    0    0    0]\n",
            " [   1   13   19   10    3  101    2    0]\n",
            " [   1   17    2    0    0    0    0    0]\n",
            " [   1    3   42    2    0    0    0    0]\n",
            " [   1    3  130    8   10  130 1032    2]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "air6zuKO4eMv"
      },
      "source": [
        "# # Load the assets and values saved in the saved model\n",
        "# Un comment this when you have trained your model and want to use layers of that and specify the file name in last line of this cell\n",
        "\n",
        "# from tensorflow.keras.models import load_model\n",
        "\n",
        "# model = tf.keras.Sequential()\n",
        "# with tf.device(devices[2]):\n",
        "#     model = load_model(\"new_model_7.h5\",custom_objects={'AttentionLayer': AttentionLayer})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kw_WSLPX4eMv",
        "outputId": "23975988-4c5e-452e-f87b-348815e27b7a"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 50)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 50, 300)      4390800     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(None, 50, 500), (N 1602000     embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, 50, 500), (N 2002000     lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 300)    1014900     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   [(None, 50, 500), (N 2002000     lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   [(None, None, 500),  1602000     embedding_1[0][0]                \n",
            "                                                                 lstm_2[0][1]                     \n",
            "                                                                 lstm_2[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "attention_layer (AttentionLayer ((None, None, 500),  500500      lstm_2[0][0]                     \n",
            "                                                                 lstm_3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concat_layer (Concatenate)      (None, None, 1000)   0           lstm_3[0][0]                     \n",
            "                                                                 attention_layer[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed (TimeDistribut (None, None, 3383)   3386383     concat_layer[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 16,500,583\n",
            "Trainable params: 16,500,583\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7lyxwPw4eMw",
        "outputId": "62bff607-31af-423a-be78-98b41b6279f3"
      },
      "source": [
        "# Fitting the model on the training set\n",
        "with tf.device(devices[2]):\n",
        "    # trained_model = model.fit([text_train, summary_train, summary_train, epochs=15,callbacks=[early_stopping],batch_size=128, validation_data=([text_test,summary_test], summary_test)    \n",
        "    trained_model = model.fit([text_train, summary_train[:,:-1]], summary_train.reshape(summary_train.shape[0],summary_train.shape[1], 1)[:,1:], epochs=15,callbacks=[early_stopping],batch_size=128, validation_data=([text_test,summary_test[:,:-1]], summary_test.reshape(summary_test.shape[0],summary_test.shape[1], 1)[:,1:]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "669/669 [==============================] - 455s 681ms/step - loss: 2.8459 - val_loss: 2.5538\n",
            "Epoch 2/15\n",
            "669/669 [==============================] - 454s 679ms/step - loss: 2.4481 - val_loss: 2.3722\n",
            "Epoch 3/15\n",
            "669/669 [==============================] - 454s 679ms/step - loss: 2.2997 - val_loss: 2.2920\n",
            "Epoch 4/15\n",
            "669/669 [==============================] - 455s 680ms/step - loss: 2.2022 - val_loss: 2.2324\n",
            "Epoch 5/15\n",
            "669/669 [==============================] - 456s 682ms/step - loss: 2.1272 - val_loss: 2.2026\n",
            "Epoch 6/15\n",
            "669/669 [==============================] - 455s 680ms/step - loss: 2.0643 - val_loss: 2.1765\n",
            "Epoch 7/15\n",
            "669/669 [==============================] - 457s 683ms/step - loss: 2.0113 - val_loss: 2.1629\n",
            "Epoch 8/15\n",
            "669/669 [==============================] - 456s 682ms/step - loss: 1.9642 - val_loss: 2.1569\n",
            "Epoch 9/15\n",
            "669/669 [==============================] - 463s 692ms/step - loss: 1.9190 - val_loss: 2.1578\n",
            "Epoch 10/15\n",
            "669/669 [==============================] - 455s 680ms/step - loss: 1.8772 - val_loss: 2.1575\n",
            "Epoch 00010: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wm-feDG54eMw"
      },
      "source": [
        "with tf.device(devices[2]):\n",
        "    model.save(\"new_model_11.h5\")   # enter the file name to save your model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDmjGO_m4eMx"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VACC-yUl4eMx"
      },
      "source": [
        "# Getting values back from saved models (Layers those were saved after training of the model)\n",
        "\n",
        "from keras import backend as K \n",
        "K.clear_session()\n",
        "\n",
        "with tf.device(devices[2]):\n",
        "    \n",
        "    # Unpacking the loaded model \n",
        "    \n",
        "    # Getting the LSTM layers from the models \n",
        "    \n",
        "    encoder_inputs = Input(shape=(max_text_length,))\n",
        "    encoder_embedding_layer = model.get_layer('embedding')(encoder_inputs)\n",
        "    \n",
        "    # encoder layer 1\n",
        "    encoder_lstm_layer1 = model.get_layer('lstm')\n",
        "    encoder_output1, state_h_layer1, state_c_layer1 = encoder_lstm_layer1(encoder_embedding_layer)\n",
        "\n",
        "    # encoder layer 2\n",
        "    encoder_lstm_layer2 = model.get_layer('lstm_1')\n",
        "    encoder_output2, state_h_layer2, state_c_layer2 = encoder_lstm_layer2(encoder_output1)\n",
        "\n",
        "    # encoder layer 3\n",
        "    encoder_lstm_layer3 = model.get_layer('lstm_2')\n",
        "    encoder_output3, state_h_layer3, state_c_layer3 = encoder_lstm_layer3(encoder_output2)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXf6FTVJ4eMx"
      },
      "source": [
        "\n",
        "with tf.device(devices[2]):\n",
        "    \n",
        "    # encoder model\n",
        "    encoder_model = Model(inputs = encoder_inputs, outputs = [encoder_output3, state_h_layer3, state_c_layer3])\n",
        "\n",
        "    decoder_state_input_h = Input(shape=(LSTM_CELLS_COUNT,))\n",
        "    decoder_state_input_c = Input(shape=(LSTM_CELLS_COUNT,))\n",
        "    decoder_hidden_state_input = Input(shape=(max_text_length,LSTM_CELLS_COUNT))\n",
        "\n",
        "    decoder_embedding_2 = model.get_layer('embedding_1')(decoder_inputs)\n",
        "    decoder_output_2 , decoder_state_h_2, decoder_state_c_2 = model.get_layer('lstm_3')(decoder_embedding_2, initial_state = [decoder_state_input_h, decoder_state_input_c])\n",
        "\n",
        "    # attention inference\n",
        "\n",
        "    attention_output_inference, attention_states_infernce = model.get_layer('attention_layer')([decoder_hidden_state_input, decoder_output_2])\n",
        "    decoder_inference_concat = Concatenate(axis=-1, name='concat')([decoder_output_2, attention_output_inference])\n",
        "\n",
        "    # dense layer  \n",
        "\n",
        "    decoder_output_2 = model.get_layer('time_distributed')(decoder_inference_concat)\n",
        "\n",
        "    # decoder model\n",
        "\n",
        "    decoder_model = Model([decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c], \n",
        "                         [decoder_output_2 ] + [decoder_state_h_2, decoder_state_c_2],name='decoder_output_p1')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-vR5kvG4eMx"
      },
      "source": [
        "# Decode the sequence "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2N3jjoHu4eMx"
      },
      "source": [
        "# This reverse Dictionaries will have word as a value for given number \n",
        "text_int_to_word = text_tokenizer.index_word\n",
        "summary_int_to_word = summary_tokenizer.index_word\n",
        "word_index_summary= summary_tokenizer.word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxohAdI-4eMy"
      },
      "source": [
        "# convert sequences of integers of summaries to original summaries\n",
        "\n",
        "def sequence_to_summary(input_sequence):\n",
        "    answer = ''\n",
        "\n",
        "    start  = word_index_summary['strtok']\n",
        "    end  = word_index_summary['endtok']\n",
        "\n",
        "    for number in input_sequence:\n",
        "        if(number!=0 and number != start and number != end):\n",
        "            answer += summary_int_to_word[number] + ' '\n",
        "    return answer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9WRVBQd4eMy"
      },
      "source": [
        "# convert sequences of integers of summaries to original summaries\n",
        "\n",
        "def sequence_to_text(input_sequence):\n",
        "    answer = ''\n",
        "    for number in input_sequence:\n",
        "        if(number!=0):\n",
        "            answer += text_int_to_word[number] + ' '\n",
        "    return answer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3G-8gCZ4eMy"
      },
      "source": [
        "# decode the sequence for answer\n",
        "# target[0,0] == word_index_summary['endtok']\n",
        "def decode_the_sequence(vec):\n",
        "\n",
        "    encoder_output, state_h , state_c = encoder_model.predict(vec)\n",
        "\n",
        "#     print(\"here =\",encoder_output)\n",
        "    target = np.zeros(shape = (1,1))\n",
        "\n",
        "    decode_sum = ''\n",
        "\n",
        "    count = 0\n",
        "    target[0,0] =  word_index_summary['strtok']\n",
        "\n",
        "    while not ( target[0,0] == word_index_summary['endtok'] or count > max_summary_length):\n",
        "\n",
        "        decoder_output, decoder_state_h, decoder_state_c = decoder_model.predict([target] + [encoder_output, state_h , state_c]) \n",
        "        \n",
        "#         print(decoder_output)\n",
        "        \n",
        "        next_index = np.argmax(decoder_output[0, -1, :])\n",
        "#         if next_index ==0:\n",
        "#             next_index=5\n",
        "        if next_index == word_index_summary['endtok']:\n",
        "            break\n",
        "#         print(next_index)\n",
        "        decode_sum+=summary_int_to_word[next_index] + ' '\n",
        "\n",
        "#         print(decode_sum)\n",
        "\n",
        "        target[0,0] = next_index\n",
        "\n",
        "        state_h , state_c = decoder_state_h, decoder_state_c\n",
        "        count+=1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return decode_sum\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5PzPJnf4eMy",
        "outputId": "11ec4787-c17f-43f5-9456-9d784671a36b"
      },
      "source": [
        "for i in range(1950,2050):\n",
        "#     print(text_train[i])\n",
        "    print(\"Review:\",sequence_to_text(text_test[i]))\n",
        "    print(\"Original summary:\",sequence_to_summary(summary_test[i]))\n",
        "    print(\"Predicted summary:\",decode_the_sequence(text_test[i].reshape(1,max_text_length)))\n",
        "#      print(\"Predicted new model:\",decode_the_sequence(text_train[i].reshape(1,max_text_length)))\n",
        "#     print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Review: taste good br price good br br upset stomach one br healthy comparison products really healthy br br bars good quick snack fresh food available let us face even kashi adds lot artificial ingredients keep flavor preservation \n",
            "Original summary: decent bars \n",
            "Predicted summary: great product \n",
            "Review: throw piece junk year service warranty nobody area second time breaks first time fixed made pay shipping charges ended expensive buy box packaging materials broke nothing terrible product even worse customer service \n",
            "Original summary: a lemon do not waste your money \n",
            "Predicted summary: do not buy \n",
            "Review: hell red best commercially available salsa market tastes fresh packs ton flavor prefer mild husband likes hot perfect adding special dishes e g guacamole dip king ranch chicken casserole etc \n",
            "Original summary: best salsa \n",
            "Predicted summary: the best \n",
            "Review: usually big fan herbal teas beverages seriously disgusting drink could would brewed actually take sip smelling odor even five year old also likes herbal beverages smelled said would want drink correct one sip enough would never ever pay money drink would happy never another taste \n",
            "Original summary: nasty \n",
            "Predicted summary: not so good \n",
            "Review: tried almost every sweetener market raise blood sugar taste best little expensive side worth sweet sugar use approximately double would use sugar bad taste br br thanks amazon caring product \n",
            "Original summary: zero wholesome sweetener \n",
            "Predicted summary: best sugar free syrup \n",
            "Review: use product prepare biscuits works excellent alternative wheat difficult digest grains \n",
            "Original summary: excellent product \n",
            "Predicted summary: great product \n",
            "Review: love earl grey tea love try stash double bergamot stash happens favorite earl grey tea convenient ones came double bergamot easy find stores drink every morning ordered supply last long time much less expensive way \n",
            "Original summary: earl grey lovers \n",
            "Predicted summary: the best tea ever \n",
            "Review: opened day day covered mold great tasting day throw two sticks eaten waste \n",
            "Original summary: better eat it fast \n",
            "Predicted summary: not a fan \n",
            "Review: two love toy dino always top toy list stand aggressive chewing ordering second one \n",
            "Original summary: it is a favorite \n",
            "Predicted summary: my dog loves it \n",
            "Review: typical pretzel eat much pumpernickel took half bag get used liked like comes pack regular sized packs hoping snack size packs anyway best thing good fer ya different flavor always good play palette little world peace \n",
            "Original summary: an acquired taste then it is good \n",
            "Predicted summary: good but not great \n",
            "Review: best easiest bread machine mix come across incredibly simple directions impossible fail plus loafs come perfect everytime issues past getting whole wheat flours rise well white flours purchased collection several times continue \n",
            "Original summary: best and easiest bread ever \n",
            "Predicted summary: best bread mix \n",
            "Review: must try apples fell love apples bought bag lego land got home looked internet see could get sweet chewy others full flavor crunchy addicted signing automatic delivery \n",
            "Original summary: buy these apples \n",
            "Predicted summary: delicious \n",
            "Review: quaker soft baked bars quickly become favorite morning snack little bars nutritious leave feeling full lunch time pecan bread flavor also worth try \n",
            "Original summary: great morning snack \n",
            "Predicted summary: great snack \n",
            "Review: think name bit misleading sugar listed ingredient honey maybe called peanut butter lots sugar honey br br also considerably sweeter plain natural grams sugar per tbsp vs natural \n",
            "Original summary: too much sugar \n",
            "Predicted summary: not what i expected \n",
            "Review: getting bonus packs consist individual packs total individual packs instead receive individual snack sized packs item packaged incorrectly amazon give big headache trying work customer service \n",
            "Original summary: do not order this \n",
            "Predicted summary: wrong item \n",
            "Review: longer make chili large chunks meat large chunks tomatoes two kinds beans right amount bite \n",
            "Original summary: awesome \n",
            "Predicted summary: great chili \n",
            "Review: good buy pretty fresh br br used get living germany explained son difference brands germany br german ones little chewier got used germany think preference based experience br later ordered fish gummi son fish loved think shape available anyones \n",
            "Original summary: gummi bear from haribo \n",
            "Predicted summary: the best \n",
            "Review: pieces awfully small maybe ears much chew dog maybe one good bite \n",
            "Original summary: not like picture \n",
            "Predicted summary: not what i expected \n",
            "Review: say yuk horrible tasting even taste like coffee bitter taste br br words come \n",
            "Original summary: yuk \n",
            "Predicted summary: bitter \n",
            "Review: good coffee keurig pod little thin side taste bust still decent cup joe \n",
            "Original summary: good coffee \n",
            "Predicted summary: good coffee \n",
            "Review: sure changed food poor dog nothing sick food used well noticed whenever feed stomach growls gets sick diarrhea would recommend get better food started looking around sounds like food caused lot illness sometimes dogs \n",
            "Original summary: this food is terrible \n",
            "Predicted summary: my dog loves it \n",
            "Review: far best coffee ever want addicted particular brand flavor try otherwise hooked also tried cinnamon blend care much seemed strong cinnamon flavor love cinnamon check prices website first though may cheaper \n",
            "Original summary: amazing \n",
            "Predicted summary: best coffee ever \n",
            "Review: k cups pretty bad tasting coffe atleast one right bitter light dark like one one starbucks pretty much amazon prime fav morning drink \n",
            "Original summary: of the only k kups i like \n",
            "Predicted summary: great taste \n",
            "Review: cappuccino tastes good ever bought coffee shop love french vanilla \n",
            "Original summary: mm mm good \n",
            "Predicted summary: my favorite coffee \n",
            "Review: purchase go store amazon purchase br fresh much expensive online \n",
            "Original summary: purchase at store \n",
            "Predicted summary: great product \n",
            "Review: good cook comes pot luck dishes came like brick sure fault roast lb hog baking things like fall next time add differnet kinds meat drain meat fyi precooked noodles boil \n",
            "Original summary: it is good \n",
            "Predicted summary: great flavor \n",
            "Review: finally found alternative gf mac n cheese kids sure miss kraft every stumbled upon alternative gf happy least good gf mac n cheese seem different regular kraft version noodles look like however cheese slightly different tasting bad exactly kraft add extra deli american cheese seems help \n",
            "Original summary: great alternative \n",
            "Predicted summary: the best mac n cheese \n",
            "Review: would given stars tin spout poorly designed precious oil spills place \n",
            "Original summary: tastes great lots of \n",
            "Predicted summary: not as advertised \n",
            "Review: makes great cup coffee strong perfect morning brew get day started \n",
            "Original summary: and flavorful \n",
            "Predicted summary: great coffee \n",
            "Review: tried several flavors coconut water pretty good choco holic high hopes zico chocolate coconut water disappointed water tastes like liquified coco puffs cereal hint coconut maybe sounds gross really pretty good refreshing \n",
            "Original summary: coco puffs \n",
            "Predicted summary: tastes like plastic \n",
            "Review: tea delicious flavorful aroma inviting really require use sugar drink require sugar tea usually something natural tea makes palatable drink without sugar goes teas tried tea fruit mix \n",
            "Original summary: good tea \n",
            "Predicted summary: great tea \n",
            "Review: kitten loves food upset tummy like wellness kitten food would recommend \n",
            "Original summary: kitten loves it \n",
            "Predicted summary: my cat loves it \n",
            "Review: good things come small packages excellent non soda thirst quencher sugar outstanding taste \n",
            "Original summary: excellent \n",
            "Predicted summary: great taste \n",
            "Review: one purple color nice pops striking white color purple undertones br br great flavor nice light also quite large bag shocked never seemed go stale br br great purchase coupled stir crazy popcorn popper \n",
            "Original summary: valley farms amish gourmet popping corn \n",
            "Predicted summary: great taste \n",
            "Review: drink zip really tasty happy dissolves easily vitamins zero calories sugar free convenient keep hand work spruce plain bottle water want flavor stix really convenient travel get bottle water anywhere add tea kick \n",
            "Original summary: tasty and convenient \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Predicted summary: great product \n",
            "Review: favorite granola hands weird processed taste like many foods days prefect combination chewy crunchy sweet plus healthy snack vanilla yogurt meal \n",
            "Original summary: best granola ever \n",
            "Predicted summary: delicious \n",
            "Review: toy great able car seat stroller son loves play things grab hold different colors textures make great time \n",
            "Original summary: great toy \n",
            "Predicted summary: my dog loves these \n",
            "Review: way better hot sauce ordered past thank carrying product \n",
            "Original summary: texas hot sauce \n",
            "Predicted summary: hot hot hot \n",
            "Review: bought product reading reviews elsewhere see improvement using cream use morning anything eyes seem swollen sorry \n",
            "Original summary: no results \n",
            "Predicted summary: not as good as i wanted \n",
            "Review: pearls boil perfect every time even better boba drinks get boba tea shops highly recommended \n",
            "Original summary: perfect every time \n",
            "Predicted summary: perfect for a \n",
            "Review: greyhounds go absolutely bananas chews give rawhide great teeth really hard made nothing sweet potato goodness sweet potatoes rich vitamins c fiber among things much healthier anyway buy time cannot recommend highly enough \n",
            "Original summary: looks like i the \n",
            "Predicted summary: my dog loves these \n",
            "Review: got wayyyy faster thought nice love fact get much one flavor husband like colors nice get lot one kind \n",
            "Original summary: awesome \n",
            "Predicted summary: good stuff \n",
            "Review: used powder years great smoothies protein shakes mixed w water used toast sandwiches graham crackers worth try pb lover want save fat calories br fyi bell plantation also sells chocolate pb like flavor \n",
            "Original summary: great stuff \n",
            "Predicted summary: great product \n",
            "Review: first tried bundle gluten free products fresh decided buy bulk expiration date three months ended given away time eat packages received taste ok sweet found getting sick fast \n",
            "Original summary: get s old too quickly \n",
            "Predicted summary: stale \n",
            "Review: thai peanut flavor hot n spicy vegan jerky winner spice nice great addition meal \n",
            "Original summary: yummy \n",
            "Predicted summary: great spicy snack \n",
            "Review: peanut butter fat regular peanut butter could believe tasted like peanut butter reconstituted water add flavors touches like way arrives mix yogurt smoothies well \n",
            "Original summary: great tasting low cal peanut butter \n",
            "Predicted summary: peanut butter peanut butter \n",
            "Review: vanilla beans great price arrived quickly airtight sealed package beans moist fragrant could happier purchase \n",
            "Original summary: great beans at a great price \n",
            "Predicted summary: great beans \n",
            "Review: received product timely manner quality excellent brand new promised problem \n",
            "Original summary: cake \n",
            "Predicted summary: great product \n",
            "Review: mile chai wonderful right balance spiciness tried chai bit spicy one well balanced great almond milk bonus points organic much one trust organic label br br compare peet chai like mile better imo peet little much spicy side although still like peet may prefer flavor profile \n",
            "Original summary: awesome chai \n",
            "Predicted summary: delicious chai \n",
            "Review: rarely say gf product tastes better gluten version cookies though melt mouth delicious right amount lemon confident tht gave gluten eaters would able tell difference \n",
            "Original summary: fantastic \n",
            "Predicted summary: great gluten free option \n",
            "Review: excellent taste reasonable calorie reorder product regular basis \n",
            "Original summary: quick solid \n",
            "Predicted summary: great product \n",
            "Review: make frosting spectrum palm oil shortening keep dairy free absolutely delicious used cakes cupcakes well frost cookies love think even better non gluten free dairy free frosting yum \n",
            "Original summary: delicious frosting \n",
            "Predicted summary: delicious \n",
            "Review: children strawberry banana crisps loved started pick strawberry pieces saw plain strawberry option thrilled arrived shocked find unlike strawberry banana variety nicely sliced pieces strawberries whole strawberries also unlike strawberries strawberry banana combination strawberries bitter mean inedible bitter word caution amazon take returns grocery items \n",
            "Original summary: do not buy the plain strawberry \n",
            "Predicted summary: not a fan \n",
            "Review: great snack bought try ordering taste somewhat like cross jack parmesan cheese get dime sized fat discs crispy baked cheese salty love \n",
            "Original summary: crispy salty yum \n",
            "Predicted summary: great snack \n",
            "Review: bad although think pods pretty much taste add \n",
            "Original summary: pretty good \n",
            "Predicted summary: not good \n",
            "Review: want second cup coffee love half caff right amount caffeine without jitters \n",
            "Original summary: nd cup \n",
            "Predicted summary: great coffee \n",
            "Review: better chocolate mix along stevia yogurt cottage cheese almond butter whatever think also makes awesome brownies \n",
            "Original summary: tasty \n",
            "Predicted summary: delicious \n",
            "Review: although love numi teas flavor tea bags weak aged earl gray keep first shipment though look brand flavor \n",
            "Original summary: weak flavor \n",
            "Predicted summary: not a good tea \n",
            "Review: wonderful find looking chicken duck breast strips ran across fantastic top read ingredients list items listed sweet potato chicken great \n",
            "Original summary: fantastic unexpected find \n",
            "Predicted summary: the best \n",
            "Review: well maybe taste buds anything else tea really tastes like milk chocolate sounds weird true least tongue love teas \n",
            "Original summary: choco peach \n",
            "Predicted summary: not bad \n",
            "Review: loved nestles diet low cal hot chocolate marshmellows calories stopped making find anywhere best \n",
            "Original summary: where have all the gone \n",
            "Predicted summary: great product \n",
            "Review: bought months ago twice price unfortunately pay even though good make scratch \n",
            "Original summary: the price is ridiculous \n",
            "Predicted summary: too expensive \n",
            "Review: agree many posters earl grey favorite tea tried every brand ever come across completely floored good tea absolutely best earl grey ever includes pricey gourmet blends bargain tins less waste time anything else \n",
            "Original summary: the best earl grey there is \n",
            "Predicted summary: the best earl grey \n",
            "Review: far best cola flavoring found wanting make cola got stevia still taste like coca cola much better nasty cola flavor sodastream makes hey artificial sweeteners always good br br taste quite like cola close enough make happy liek flavor \n",
            "Original summary: best so far \n",
            "Predicted summary: not the best \n",
            "Review: whoa really nice sweetness softer raw organic cane sugar presently using aftertaste never could get used stevia coffee delicious good product \n",
            "Original summary: great taste \n",
            "Predicted summary: great taste \n",
            "Review: received quaker soft baked oatmeal cookie chocolate almond box post saw receiving baffled even though cookie individually wrapped cookies fresh enough open lack crispness due flavor product lost relish cookie much would loved \n",
            "Original summary: not good \n",
            "Predicted summary: not so good \n",
            "Review: really like cereal use water try milk super tasty agree needs bigger box go box days \n",
            "Original summary: yum \n",
            "Predicted summary: great breakfast cereal \n",
            "Review: getting san francisco bay rainforest blend coffee months k cups become one favorites usually drink french roasts really enjoy blend href http www amazon com gp product b san francisco bay coffee one cup rainforest blend count \n",
            "Original summary: delicious coffee \n",
            "Predicted summary: great coffee \n",
            "Review: best maple syrup tasted perfect balance sweetness maple flavor kids normally use maple syrup strong taste tried several love one great mild maple flavor right amount sweetness glad tried \n",
            "Original summary: lish \n",
            "Predicted summary: great syrup \n",
            "Review: tastes great doubt good much sugar everyone family loves cannot buy time eat much one best tasting cereals ever imo \n",
            "Original summary: very good cereal \n",
            "Predicted summary: best cereal ever \n",
            "Review: kid remember favorites fruit loops lucky charms kix mother made golden crisp get older cereals stop tasting good hence sugar br br found new adult cereal diabetic enjoy strawberries fall love \n",
            "Original summary: found my new adult cereal \n",
            "Predicted summary: my favorite cereal \n",
            "Review: husband loves product rich creamy something drinks day long every day became longer available local grocery stores know going rehab perhaps amazon website came rescue amazon store ship us free shipping case often need thank amazon thank folgers wonderful product please ever stop making \n",
            "Original summary: folgers cappuccino addiction \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Predicted summary: great product \n",
            "Review: completely agree previous review outstanding coffee tried lot k cup varieties found one best wonderfully smooth flavor really superior cup coffee \n",
            "Original summary: great coffee \n",
            "Predicted summary: best coffee ever \n",
            "Review: love taste dont know drink bad reviews personally like much n e coconut water \n",
            "Original summary: best coconut water i have ever had \n",
            "Predicted summary: tastes like plastic \n",
            "Review: originally unhappy order appeared dead complain got back sent replacement gave trouble culture grew well since several months well pleased defense pennsylvania earthquake strange weather first go around \n",
            "Original summary: easy to use \n",
            "Predicted summary: great for cooking \n",
            "Review: purchased item anxious see results eyes dark puffy received product excited realized pump work squeezed cream seems work fine however seen results yet going previous reviews reason purchased product \n",
            "Original summary: pump does not work \n",
            "Predicted summary: not as advertised \n",
            "Review: purchased company years great prices trust quality teas best available hands company truly care customers \n",
            "Original summary: company great quality and great price \n",
            "Predicted summary: great tea \n",
            "Review: package opened gone hard tasty gf hoarding necessary enough cookies share package \n",
            "Original summary: habit \n",
            "Predicted summary: not bad \n",
            "Review: ok one tell son actually put baby food always use organic muffins even able tell br easy mix batter change color br even cannot get eat things like sweet potatoes getting anyway mommy \n",
            "Original summary: good product \n",
            "Predicted summary: baby \n",
            "Review: italian greyhound digestive system delicate used frequent eye stinky farts switched food nice firm poops \n",
            "Original summary: good stuff \n",
            "Predicted summary: great \n",
            "Review: possibly best jerky ever tasted friends sampled agreed moist easy chew bonus us looked almost like freshly cooked steak strips look processed excellent every way except price high purchase unfortunately \n",
            "Original summary: steak to go \n",
            "Predicted summary: best jerky ever \n",
            "Review: used tropical medley years orange glazed tropical fruit scones pillsbury bake recipe fran fruit mixes work well br br thank fran wherever may \n",
            "Original summary: excellent for baking \n",
            "Predicted summary: great taste \n",
            "Review: tried reviewing another switch product black cherry flavor kiwi berry also tasty though life could taste kiwi strawberry br br kiwi berry problem black cherry however much sugar ounce ounce much sugar regular non diet soda would definitely drink switch products try convince health drink \n",
            "Original summary: yummy but where is the kiwi \n",
            "Predicted summary: great tasting \n",
            "Review: mccann instant irish oatmeal variety pack regular apples cinnamon maple brown sugar count boxes pack br br fan mccann steel cut oats thought would give instant variety try found hardy meal sweet great folks like post bariatric surgery need food palatable easily digestible fiber make bloat \n",
            "Original summary: satisfying \n",
            "Predicted summary: good stuff \n",
            "Review: cats absolutely love avoderm chicken duck definitely preferences within avoderm products pet mom happy see brands like avoderm taking use wholesome real food ingredients products proud avoderm supporter really see difference cats energy level shiny soft fur since switching avoderm \n",
            "Original summary: my cats go crazy for this \n",
            "Predicted summary: my cats love it \n",
            "Review: arrived time dont quite melt advertised required much left something sludge bottom cup \n",
            "Original summary: they sort of work \n",
            "Predicted summary: not as described \n",
            "Review: tea tastes fine br know long br tea bags came loose cardboard box br must institutional packaging br see lasts \n",
            "Original summary: bags not \n",
            "Predicted summary: good tea \n",
            "Review: fell love coca tea research highland peru love available us reviewer right really distinctive delicate taste also prefer piping hot little sugar although husband wants try iced must tea drinkers quite lovely gives nice pick like caffeine \n",
            "Original summary: so much better than coffee \n",
            "Predicted summary: the best tea ever \n",
            "Review: granola mix rolled oats semi sweet chocolate chunks could find description bag whether snack cereal snack bit dry taste sweet delicious quaker natural quite enjoyed milk crunchy texture right chocolate treat like granola good one good source fiber \n",
            "Original summary: tasty \n",
            "Predicted summary: tasty and healthy \n",
            "Review: taste like roasted marshmallows sweet potato casserole thanksgiving awesome issue w cannot stop eating \n",
            "Original summary: so good \n",
            "Predicted summary: best snack ever \n",
            "Review: chocolate compares little taste bud smooth outer shell incomparable flavor releasing even inner cream mouth explosion total chocolate bliss chocolates colder temps reduce flavor cold weather make sure warmed least degrees get best chocolate hit great stuff \n",
            "Original summary: best chocolate on the planet \n",
            "Predicted summary: chocolate chocolate \n",
            "Review: tea simply amazing definitely exotic flavor good taste especially taste best drink tea feel sensations licorice mint taste incredible must drink \n",
            "Original summary: wow \n",
            "Predicted summary: the best tea ever \n",
            "Review: picked bars favorite larabars package arrived noticed even though still months till expiration date bars stale taste one usually getting grocery store \n",
            "Original summary: disappointed \n",
            "Predicted summary: stale \n",
            "Review: product expected spicy smokey flavor either super light regular store paprika flavor \n",
            "Original summary: spicy smoked paprika \n",
            "Predicted summary: spicy \n",
            "Review: san francisco bay fog chaser provide pretty decent bold coffee phenomenal price half price common competitors well worth ordering like spend x something else next time bet satisfied fog chaser \n",
            "Original summary: good coffee great price \n",
            "Predicted summary: great value \n",
            "Review: picky dog even likes easy break training purposes order \n",
            "Original summary: dogs love em \n",
            "Predicted summary: my dog loves these \n",
            "Review: discovered ganoderma friend found particular brand via amazon search really like smoother brands tried great taste gives pick need complaint packets impossible tear use scissors cut order pour powdered coffee little slit edge would make huge difference \n",
            "Original summary: smooth tasty energy packed \n",
            "Predicted summary: great taste \n",
            "Review: outstanding dressing usually make enjoy dressings superior use salads meat dipping sauce thank much selling outstanding product \n",
            "Original summary: superior \n",
            "Predicted summary: great dressing \n",
            "Review: good price starbucks k cups tired paying lots grocery store staples instead stock coffee instead week like convenience \n",
            "Original summary: good buy \n",
            "Predicted summary: great value \n",
            "Review: good right green food item three year old mouth however bit salty buy way cheaper asian food grocery \n",
            "Original summary: good but \n",
            "Predicted summary: good but too salty \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GG1QJRgz4eMz"
      },
      "source": [
        "# from rouge_score import rouge_scorer\n",
        "# # # scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "# # # scores = scorer.score('The quick brown fox jumps over the lazy dog',\n",
        "# # #                       'The quick brown dog jumps on the log.')\n",
        "# for i in range(100):\n",
        "#     scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "#     scores = scorer.score(o[i],p[i])\n",
        "#     print(\"\\n\",scores)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}